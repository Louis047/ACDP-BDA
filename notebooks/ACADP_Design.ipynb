{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACADP: Adaptive Correlation-Aware Differential Privacy\n",
    "## Code Review Notebook\n",
    "\n",
    "**Team Workflow Distribution:**\n",
    "- Workflow 1: Data Ingestion & Preprocessing (Teammate 1)\n",
    "- Workflow 2: Correlation & Feature Grouping (Teammate 2)\n",
    "- Workflow 3: Differential Privacy & Budget Allocation (You)\n",
    "- Workflow 4: Evaluation & Validation (Teammate 3)\n",
    "\n",
    "**Dataset:** NYC Taxi Trip Data (Jan 2023)  \n",
    "**Global Privacy Budget:** ε = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## WORKFLOW 1: Data Ingestion & Preprocessing\n",
    "**Owner:** Teammate 1\n",
    "\n",
    "**Modules:**\n",
    "- Batch data loading with PySpark\n",
    "- Schema validation and feature typing\n",
    "- Missing value handling\n",
    "- Feature bounding (DP requirement)\n",
    "- Normalization\n",
    "\n",
    "**Deliverable:** Clean, bounded dataset with feature metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install pyspark -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NYC Taxi data (single file to avoid schema conflicts)\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "os.makedirs(\"nyc_taxi\", exist_ok=True)\n",
    "\n",
    "base_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/\"\n",
    "filename = \"yellow_tripdata_2023-01.parquet\"  # Using single file\n",
    "filepath = f\"nyc_taxi/{filename}\"\n",
    "\n",
    "if not os.path.exists(filepath):\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    urllib.request.urlretrieve(base_url + filename, filepath)\n",
    "    print(\"✓ Download complete\")\n",
    "else:\n",
    "    print(f\"✓ {filename} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ACADP\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✓ Spark {spark.version} initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = spark.read.parquet(\"nyc_taxi/yellow_tripdata_2023-01.parquet\")\n",
    "\n",
    "print(f\"✓ Loaded {df.count():,} rows, {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View schema\n",
    "print(\"Schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast numeric columns to double for consistency\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "numeric_cols = [\n",
    "    \"passenger_count\", \"trip_distance\", \"fare_amount\",\n",
    "    \"extra\", \"mta_tax\", \"tip_amount\",\n",
    "    \"tolls_amount\", \"total_amount\"\n",
    "]\n",
    "\n",
    "for col_name in numeric_cols:\n",
    "    if col_name in df.columns:\n",
    "        df = df.withColumn(col_name, col(col_name).cast(\"double\"))\n",
    "\n",
    "print(\"✓ Numeric columns cast to double\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "df = df.dropna(subset=[\n",
    "    \"passenger_count\", \"trip_distance\", \"fare_amount\", \n",
    "    \"tip_amount\", \"total_amount\"\n",
    "])\n",
    "\n",
    "print(f\"✓ After cleaning: {df.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature bounding (DP requirement)\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Define bounds\n",
    "bounds = {\n",
    "    \"passenger_count\": (1, 6),\n",
    "    \"trip_distance\": (0, 100),\n",
    "    \"fare_amount\": (0, 500),\n",
    "    \"extra\": (0, 100),\n",
    "    \"mta_tax\": (0, 10),\n",
    "    \"tip_amount\": (0, 100),\n",
    "    \"tolls_amount\": (0, 50),\n",
    "    \"total_amount\": (0, 600)\n",
    "}\n",
    "\n",
    "# Apply bounds\n",
    "for feature, (min_val, max_val) in bounds.items():\n",
    "    if feature in df.columns:\n",
    "        df = df.withColumn(feature, \n",
    "                           when(col(feature).isNull(), min_val)\n",
    "                           .when(col(feature) < min_val, min_val)\n",
    "                           .when(col(feature) > max_val, max_val)\n",
    "                           .otherwise(col(feature)))\n",
    "\n",
    "print(\"✓ Feature bounding applied\")\n",
    "df.select(\"fare_amount\", \"trip_distance\", \"passenger_count\", \"tip_amount\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data\n",
    "df.write.mode(\"overwrite\").parquet(\"nyc_taxi_preprocessed\")\n",
    "print(\"✓ WORKFLOW 1 COMPLETE: Preprocessed data saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## WORKFLOW 2: Correlation & Feature Grouping\n",
    "**Owner:** Teammate 2\n",
    "\n",
    "**Modules:**\n",
    "- Approximate Pearson correlation (sampled)\n",
    "- Discretized Mutual Information\n",
    "- Sparse dependency extraction\n",
    "- Graph-based feature grouping (connected components)\n",
    "\n",
    "**Deliverable:** Privacy blocks (correlated feature groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional dependencies\n",
    "!pip install networkx scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload preprocessed data\n",
    "df = spark.read.parquet(\"nyc_taxi_preprocessed\")\n",
    "\n",
    "# Select numerical features\n",
    "all_num_cols = [\n",
    "    \"passenger_count\", \"trip_distance\", \"fare_amount\",\n",
    "    \"extra\", \"mta_tax\", \"tip_amount\", \n",
    "    \"tolls_amount\", \"total_amount\"\n",
    "]\n",
    "\n",
    "num_cols = [c for c in all_num_cols if c in df.columns]\n",
    "\n",
    "print(f\"Analyzing {len(num_cols)} numerical features: {num_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample for correlation computation (1% sample)\n",
    "SAMPLE_FRACTION = 0.01\n",
    "sample_df = df.select(num_cols).sample(fraction=SAMPLE_FRACTION, seed=42)\n",
    "\n",
    "print(f\"✓ Sample: {sample_df.count():,} rows ({SAMPLE_FRACTION*100}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Pearson correlations\n",
    "from itertools import combinations\n",
    "\n",
    "PEARSON_THRESH = 0.4\n",
    "pearson_pairs = []\n",
    "\n",
    "print(f\"Computing Pearson correlations (threshold = {PEARSON_THRESH})...\")\n",
    "for f1, f2 in combinations(num_cols, 2):\n",
    "    corr = sample_df.stat.corr(f1, f2)\n",
    "    if corr and abs(corr) >= PEARSON_THRESH:\n",
    "        pearson_pairs.append((f1, f2, corr))\n",
    "\n",
    "print(f\"✓ Found {len(pearson_pairs)} significant correlations\\n\")\n",
    "\n",
    "# Top correlations\n",
    "for f1, f2, corr in sorted(pearson_pairs, key=lambda x: abs(x[2]), reverse=True)[:5]:\n",
    "    print(f\"  {f1:20s} <-> {f2:20s} : {corr:>6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Mutual Information\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "sample_pd = sample_df.toPandas()\n",
    "MI_THRESH = 0.1\n",
    "mi_pairs = []\n",
    "\n",
    "print(f\"Computing Mutual Information (threshold = {MI_THRESH})...\")\n",
    "for target_col in num_cols:\n",
    "    feature_cols = [c for c in num_cols if c != target_col]\n",
    "    X = sample_pd[feature_cols].values\n",
    "    y = sample_pd[target_col].values\n",
    "    \n",
    "    mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "    \n",
    "    for i, feature_col in enumerate(feature_cols):\n",
    "        if mi_scores[i] >= MI_THRESH:\n",
    "            pair = tuple(sorted([feature_col, target_col]))\n",
    "            mi_pairs.append((*pair, mi_scores[i]))\n",
    "\n",
    "mi_pairs = list(set(mi_pairs))\n",
    "print(f\"✓ Found {len(mi_pairs)} significant MI dependencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Pearson and MI edges\n",
    "all_edges = set()\n",
    "\n",
    "for f1, f2, _ in pearson_pairs:\n",
    "    all_edges.add(tuple(sorted([f1, f2])))\n",
    "\n",
    "for f1, f2, _ in mi_pairs:\n",
    "    all_edges.add(tuple(sorted([f1, f2])))\n",
    "\n",
    "print(f\"✓ Combined: {len(all_edges)} unique dependency edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dependency graph and find feature blocks\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(num_cols)\n",
    "\n",
    "for f1, f2 in all_edges:\n",
    "    G.add_edge(f1, f2)\n",
    "\n",
    "feature_blocks = [sorted(list(block)) for block in nx.connected_components(G)]\n",
    "\n",
    "print(f\"✓ Feature grouping: {len(feature_blocks)} privacy blocks\\n\")\n",
    "for i, block in enumerate(feature_blocks):\n",
    "    print(f\"Block {i} ({len(block)} features): {block}\")\n",
    "\n",
    "print(\"\\n✓ WORKFLOW 2 COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## WORKFLOW 3: Differential Privacy & Budget Allocation\n",
    "**Owner:** You (Primary)\n",
    "\n",
    "**Modules:**\n",
    "- Joint sensitivity estimation per block\n",
    "- Adaptive ε allocation (data-driven, no uniform)\n",
    "- Laplace mechanism (ε-DP) at block level\n",
    "- Privacy accounting\n",
    "\n",
    "**Deliverable:** Privatized dataset with ε allocation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block-level sensitivity estimation\n",
    "import numpy as np\n",
    "\n",
    "block_sensitivities = []\n",
    "\n",
    "print(\"Block Sensitivity Estimation:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, block in enumerate(feature_blocks):\n",
    "    feature_sens = []\n",
    "    for feature in block:\n",
    "        min_val, max_val = bounds[feature]\n",
    "        sensitivity = max_val - min_val\n",
    "        feature_sens.append(sensitivity)\n",
    "    \n",
    "    # L2 block sensitivity\n",
    "    l2_sensitivity = np.sqrt(np.sum(np.array(feature_sens) ** 2))\n",
    "    \n",
    "    block_sensitivities.append({\n",
    "        'block_id': i,\n",
    "        'features': block,\n",
    "        'l2_sensitivity': l2_sensitivity\n",
    "    })\n",
    "    \n",
    "    print(f\"Block {i}: Δ = {l2_sensitivity:.2f}\")\n",
    "\n",
    "print(\"\\n✓ Sensitivity estimation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive privacy budget allocation\n",
    "EPSILON_GLOBAL = 1.0\n",
    "total_sensitivity = sum(bs['l2_sensitivity'] for bs in block_sensitivities)\n",
    "\n",
    "print(f\"Adaptive Budget Allocation (ε = {EPSILON_GLOBAL}):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for bs in block_sensitivities:\n",
    "    # Sensitivity-proportional allocation\n",
    "    epsilon_block = EPSILON_GLOBAL * (bs['l2_sensitivity'] / total_sensitivity)\n",
    "    bs['epsilon'] = epsilon_block\n",
    "    \n",
    "    print(f\"Block {bs['block_id']}: ε = {epsilon_block:.4f} ({100*epsilon_block/EPSILON_GLOBAL:.1f}%)\")\n",
    "\n",
    "total_allocated = sum(bs['epsilon'] for bs in block_sensitivities)\n",
    "print(f\"\\nTotal: {total_allocated:.6f}\")\n",
    "print(\"✓ Budget allocation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Laplace mechanism\n",
    "df_for_dp = sample_pd.copy()\n",
    "df_private = df_for_dp.copy()\n",
    "\n",
    "print(\"Applying Laplace Mechanism:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for bs in block_sensitivities:\n",
    "    features = bs['features']\n",
    "    sensitivity = bs['l2_sensitivity']\n",
    "    epsilon = bs['epsilon']\n",
    "    laplace_scale = sensitivity / epsilon\n",
    "    \n",
    "    print(f\"Block {bs['block_id']}: λ = {laplace_scale:.2f}\")\n",
    "    \n",
    "    for feature in features:\n",
    "        noise = np.random.laplace(0, laplace_scale, size=len(df_private))\n",
    "        df_private[feature] = df_private[feature] + noise\n",
    "        \n",
    "        # Clip to bounds\n",
    "        min_val, max_val = bounds[feature]\n",
    "        df_private[feature] = df_private[feature].clip(min_val, max_val)\n",
    "\n",
    "print(\"\\n✓ Differential privacy applied\")\n",
    "print(\"✓ WORKFLOW 3 COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## WORKFLOW 4: Evaluation & Validation\n",
    "**Owner:** Teammate 3\n",
    "\n",
    "**Modules:**\n",
    "- Baseline: Feature-independent uniform ε DP\n",
    "- Utility metrics: MAE, correlation preservation\n",
    "- Comparison: ACADP vs Baseline\n",
    "\n",
    "**Deliverable:** Quantitative comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: Uniform DP\n",
    "df_baseline = df_for_dp.copy()\n",
    "epsilon_per_feature = EPSILON_GLOBAL / len(num_cols)\n",
    "\n",
    "print(f\"Baseline: Uniform DP (ε/feature = {epsilon_per_feature:.4f})\")\n",
    "\n",
    "for feature in num_cols:\n",
    "    min_val, max_val = bounds[feature]\n",
    "    sensitivity = max_val - min_val\n",
    "    laplace_scale = sensitivity / epsilon_per_feature\n",
    "    \n",
    "    noise = np.random.laplace(0, laplace_scale, size=len(df_baseline))\n",
    "    df_baseline[feature] = df_baseline[feature] + noise\n",
    "    df_baseline[feature] = df_baseline[feature].clip(min_val, max_val)\n",
    "\n",
    "print(\"✓ Baseline DP applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 1: Mean Absolute Error\n",
    "mae_acadp = {f: np.mean(np.abs(df_for_dp[f] - df_private[f])) for f in num_cols}\n",
    "mae_baseline = {f: np.mean(np.abs(df_for_dp[f] - df_baseline[f])) for f in num_cols}\n",
    "\n",
    "avg_mae_acadp = np.mean(list(mae_acadp.values()))\n",
    "avg_mae_baseline = np.mean(list(mae_baseline.values()))\n",
    "\n",
    "print(\"Mean Absolute Error:\")\n",
    "print(f\"  ACADP:    {avg_mae_acadp:.2f}\")\n",
    "print(f\"  Baseline: {avg_mae_baseline:.2f}\")\n",
    "print(f\"  Improvement: {((avg_mae_baseline - avg_mae_acadp) / avg_mae_baseline * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 2: Correlation Preservation\n",
    "corr_original = df_for_dp[num_cols].corr()\n",
    "corr_acadp = df_private[num_cols].corr()\n",
    "corr_baseline = df_baseline[num_cols].corr()\n",
    "\n",
    "corr_error_acadp = np.mean(np.abs(corr_original.values - corr_acadp.values))\n",
    "corr_error_baseline = np.mean(np.abs(corr_original.values - corr_baseline.values))\n",
    "\n",
    "print(\"\\nCorrelation Preservation Error:\")\n",
    "print(f\"  ACADP:    {corr_error_acadp:.4f}\")\n",
    "print(f\"  Baseline: {corr_error_baseline:.4f}\")\n",
    "print(f\"  Improvement: {((corr_error_baseline - corr_error_acadp) / corr_error_baseline * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "sns.heatmap(corr_original, annot=False, cmap='coolwarm', center=0, \n",
    "            vmin=-1, vmax=1, ax=axes[0], cbar_kws={'label': 'Correlation'})\n",
    "axes[0].set_title('Original', fontsize=14, fontweight='bold')\n",
    "\n",
    "sns.heatmap(corr_acadp, annot=False, cmap='coolwarm', center=0,\n",
    "            vmin=-1, vmax=1, ax=axes[1], cbar_kws={'label': 'Correlation'})\n",
    "axes[1].set_title(f'ACADP (error={corr_error_acadp:.4f})', fontsize=14, fontweight='bold')\n",
    "\n",
    "sns.heatmap(corr_baseline, annot=False, cmap='coolwarm', center=0,\n",
    "            vmin=-1, vmax=1, ax=axes[2], cbar_kws={'label': 'Correlation'})\n",
    "axes[2].set_title(f'Baseline (error={corr_error_baseline:.4f})', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: correlation_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "summary = pd.DataFrame({\n",
    "    'Metric': ['Average MAE', 'Correlation Error', 'Privacy Budget (ε)'],\n",
    "    'ACADP': [\n",
    "        f\"{avg_mae_acadp:.2f}\",\n",
    "        f\"{corr_error_acadp:.4f}\",\n",
    "        f\"{EPSILON_GLOBAL}\"\n",
    "    ],\n",
    "    'Baseline': [\n",
    "        f\"{avg_mae_baseline:.2f}\",\n",
    "        f\"{corr_error_baseline:.4f}\",\n",
    "        f\"{EPSILON_GLOBAL}\"\n",
    "    ],\n",
    "    'ACADP Improvement': [\n",
    "        f\"{((avg_mae_baseline - avg_mae_acadp) / avg_mae_baseline * 100):.2f}%\",\n",
    "        f\"{((corr_error_baseline - corr_error_acadp) / corr_error_baseline * 100):.2f}%\",\n",
    "        \"N/A\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(\"\\n✓ WORKFLOW 4 COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Summary\n",
    "\n",
    "**Key Findings:**\n",
    "1. ACADP preserves correlations better than baseline uniform DP\n",
    "2. Comparable or lower error rates\n",
    "3. Adaptive budget allocation optimizes for data characteristics\n",
    "4. Block-level DP more efficient than per-feature DP\n",
    "\n",
    "**Limitations:**\n",
    "- 1% sampling for correlation analysis (computational feasibility)\n",
    "- Single month dataset (Jan 2023)\n",
    "- Thresholding: |r| ≥ 0.4, MI ≥ 0.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
